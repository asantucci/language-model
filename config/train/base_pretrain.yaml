checkpoint:
  resume: ""
  out_dir: "./checkpoints"
  checkpoint_path: "pretrain_ckpt"
  save_interval: 1000

data:
  hf_dataset_name: "wikipedia"
  hf_dataset_dir: "20220301.en"

optim:
  learning_rate: 2e-4
  min_learning_rate: 5e-5
  scheduler_type: "one_cycle"
  pct_warmup: 0.3
  adamw_beta1: 0.9
  adamw_beta2: 0.95
  adamw_weight_decay: 0.1
  adamw_use_fused: true
  use_eight_bit_optimizer: true
  grad_clip: 1.0

tokenizer:
  pretrained_tokenizer_name: "openai-community/gpt2"
  pretrained_tokenizer_max_length: 51200

train:
  batch_size: 8
  seq_len: 512
  max_train_steps: 15000
  gradient_accumulation_steps: 1
  eval_interval: 1000
  eval_iters: 5
  generate_interval: 1000
  dtype: "bfloat16"
  device: "cuda"

wandb:
  wandb_project: deepseek_early_sweep
  wandb_run_name: pretrain_medium
  wandb_log: true
  log_interval: 10
