kv_cache:
  use_kv_cache: false
  q_lora_rank: 128
  kv_lora_rank: 512
  rope_head_dim: 64
  nope_head_dim: 128
  v_head_dim: 64
  rope_base: 10000

misc:
  init_weight_std: 0.02
  rms_norm_eps: 1e-6
  device: cuda

model:
  d_model: 128
  nheads: 2
  num_layers: 2
  max_position_embeddings: 4096
  dropout: 0.1
  vocab_size: 50257

moe:
  num_shared_experts: 2
  num_routed_experts: 2
  moe_hidden_dimension: 256
  mlp_hidden_dimension: 256
  topk: 2
  expert_load_balance_factor: 0.01
  topk_norm_epsilon: 1e-9
  normalized_moe_gates: true
  first_k_dense_replace: 4
  disable_moe: false

rope:
  head_dim: 64
  base: 10000
  